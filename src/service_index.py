from config import logger
import service_chroma as chroma_service
from service_metadata import get_analysis_service
import server_lifecycle as server_lifecycle
import json
from datetime import datetime as time

def _flatten_keywords(keywords):
    """
    Flatten keywords from various formats to a comma-separated string.
    
    Handles:
    - Flat list: ["Keyword1", "Keyword2"] -> "Keyword1, Keyword2"
    - Nested dict: {"Category": ["Kw1", "Kw2"], ...} -> "Kw1, Kw2, ..."
    - Already a string: "Keyword1, Keyword2" -> "Keyword1, Keyword2"
    
    Args:
        keywords: List, dict, or string of keywords
        
    Returns:
        Comma-separated string of all keywords
    """
    if not keywords:
        return ""
    
    if isinstance(keywords, str):
        # Already a string, return as-is
        return keywords
    
    if isinstance(keywords, list):
        # Flat list of strings
        return ', '.join(str(kw) for kw in keywords if kw)
    
    if isinstance(keywords, dict):
        # Nested dict - recursively collect all keywords
        all_keywords = []
        
        def collect_keywords(d):
            for key, value in d.items():
                if isinstance(value, list):
                    # Leaf node with keywords
                    all_keywords.extend(str(kw) for kw in value if kw)
                elif isinstance(value, dict) and value:
                    # Nested dict, recurse
                    collect_keywords(value)
                else:
                    # Single keyword value
                    if value:
                        all_keywords.append(str(key))
        
        collect_keywords(keywords)
        return ', '.join(all_keywords)
    
    return ""

def process_image_task(
    image_triplets: list[tuple[bytes, str, str]], 
    options: dict
) -> tuple[int, int]:
    """
    Process a batch of images for indexing.
    
    Args:
        image_triplets: List of (image_bytes, uuid, filename) tuples
        options: Dictionary with all processing options
        
    Returns:
        Tuple of (success_count, failure_count)
    """
    success_count = 0
    failure_count = 0
    total_images = len(image_triplets)

    try:
        provider = options.get('provider')
        model_name = options.get('model')
        replace_ss = options.get('replace_ss', False)
        regenerate_metadata = options.get('regenerate_metadata', True)
        compute_embeddings = options.get('compute_embeddings', True)
        compute_metadata = options.get('compute_metadata', False)
        compute_quality = options.get('compute_quality', True)

        logger.info(f"Starting batch processing of {total_images} images...")
        logger.info(f"regenerate_metadata={regenerate_metadata}, compute_embeddings={compute_embeddings}, "
                   f"compute_metadata={compute_metadata}, compute_quality={compute_quality}")
        
        # Check existing records if regenerate_metadata is False
        existing_records = {}
        if not regenerate_metadata:
            logger.info("Checking existing records to determine what needs generation...")
            for _, uuid, _ in image_triplets:
                existing_record = chroma_service.get_image(uuid)
                if existing_record and existing_record['ids']:
                    existing_records[uuid] = existing_record['metadatas'][0] if existing_record['metadatas'] else {}
        
        # Determine what actually needs to be computed for each image
        images_needing_embeddings = []
        images_needing_metadata = []
        images_needing_quality = []
        
        for _, uuid, _ in image_triplets:
            existing = existing_records.get(uuid, {})
            
            # Check if embedding is needed
            needs_embedding = compute_embeddings and (regenerate_metadata or not existing.get('has_embedding', False))
            if needs_embedding:
                images_needing_embeddings.append(uuid)
            
            # Check if metadata is needed
            has_any_metadata = existing.get('title') or existing.get('caption') or existing.get('alt_text') or existing.get('keywords')
            needs_metadata = compute_metadata and (regenerate_metadata or not has_any_metadata)
            if existing and compute_metadata:
                logger.info(f"UUID {uuid}: has_metadata={has_any_metadata}, regenerate={regenerate_metadata}, needs_metadata={needs_metadata}")
                logger.info(f"  Existing fields: title={bool(existing.get('title'))}, caption={bool(existing.get('caption'))}, "
                          f"alt_text={bool(existing.get('alt_text'))}, keywords={bool(existing.get('keywords'))}")
            if needs_metadata:
                images_needing_metadata.append(uuid)
            
            # Check if quality scores are needed
            needs_quality = compute_quality and (regenerate_metadata or not existing.get('overall_score'))
            if needs_quality:
                images_needing_quality.append(uuid)
        
        logger.info(f"Generation needed: {len(images_needing_embeddings)} embeddings, "
                   f"{len(images_needing_metadata)} metadata, {len(images_needing_quality)} quality scores")

        # If absolutely nothing needs to be generated, treat this as a successful no-op
        if len(images_needing_embeddings) == 0 and len(images_needing_metadata) == 0 and len(images_needing_quality) == 0:
            logger.info("No generation required (regenerate_metadata=False and all fields present). Returning success without changes.")
            return len(image_triplets), 0
        
        # Override compute flags based on what's actually needed
        actual_compute_embeddings = len(images_needing_embeddings) > 0
        actual_compute_metadata = len(images_needing_metadata) > 0
        actual_compute_quality = len(images_needing_quality) > 0
        
        analysis_service = get_analysis_service()
        siglip_model = None
        siglip_processor = None
        # Lazily acquire the model & processor for this batch to avoid loading at import time.
        if actual_compute_embeddings:
            siglip_model = server_lifecycle.get_model()
            siglip_processor = server_lifecycle.get_processor()

        # Convert lists to sets for faster lookup in analyze_batch
        embeddings, datetimes, metadata_results, ratings = analysis_service.analyze_batch(
            image_triplets, options, siglip_model, siglip_processor,
            set(images_needing_embeddings), set(images_needing_metadata), set(images_needing_quality)
        )

        if embeddings is None and compute_embeddings:
            return 0, total_images

        filenames = {triplet[1]: triplet[2] for triplet in image_triplets}

        for i, (image_bytes, uuid, filename) in enumerate(image_triplets):
            try:
                embedding = embeddings[i] if embeddings is not None else None
                rating_data = ratings[i] if ratings else None
                metadata_data = metadata_results[i] if metadata_results else None
                
                existing = existing_records.get(uuid, {})
                
                need_embedding = uuid in images_needing_embeddings
                need_metadata = uuid in images_needing_metadata
                need_quality = uuid in images_needing_quality

                # Validate that required new data was generated if needed
                if need_embedding and embedding is None:
                    logger.error(f"Embedding generation failed for {uuid}. Skipping.")
                    failure_count += 1
                    continue
                
                if need_quality and (not rating_data or not rating_data.success):
                    logger.error(f"Quality rating generation failed for {uuid}. Skipping.")
                    failure_count += 1
                    continue

                if need_metadata and (not metadata_data or not metadata_data.success):
                    logger.error(f"Metadata generation failed for {uuid}. Skipping.")
                    failure_count += 1
                    continue

                # If nothing needed for this UUID (already complete) just count success and move on
                if not need_embedding and not need_metadata and not need_quality and not regenerate_metadata:
                    logger.info(f"UUID {uuid}: already fully indexed; skipping update.")
                    success_count += 1
                    continue

                # Start with existing metadata if not regenerating
                if not regenerate_metadata and existing:
                    main_metadata = existing.copy()
                    # Update only basic fields that should always be current
                    main_metadata["filename"] = filename
                    main_metadata["uuid"] = uuid
                else:
                    main_metadata = {
                        "filename": filename,
                        "uuid": uuid,
                        "provider": provider,
                        "model": model_name,
                    }

                # Update/add datetime if present
                if datetimes:
                    capture_time = datetimes.get(uuid)
                    if capture_time:
                        main_metadata["capture_time"] = capture_time

                # Update quality scores if newly generated
                if rating_data and rating_data.success:
                    main_metadata["overall_score"] = rating_data.overall_score
                    main_metadata["composition_score"] = rating_data.composition_score
                    main_metadata["lighting_score"] = rating_data.lighting_score
                    main_metadata["motiv_score"] = rating_data.motiv_score
                    main_metadata["colors_score"] = rating_data.colors_score
                    main_metadata["emotion_score"] = rating_data.emotion_score
                    main_metadata["quality_critique"] = rating_data.critique
                    main_metadata["provider"] = provider
                    main_metadata["model"] = model_name

                # Update metadata fields if newly generated
                if metadata_data and metadata_data.success:
                    if metadata_data.title:
                        main_metadata['title'] = metadata_data.title
                    if metadata_data.caption:
                        main_metadata['caption'] = metadata_data.caption
                    if metadata_data.alt_text:
                        main_metadata['alt_text'] = metadata_data.alt_text
                    if metadata_data.keywords:
                        main_metadata['keywords'] = json.dumps(metadata_data.keywords)
                        #logger.debug(f"UUID {uuid}: keywords JSON data: {main_metadata['keywords']}")
                        main_metadata['flattened_keywords'] = _flatten_keywords(metadata_data.keywords)
                    if not main_metadata.get("provider"):
                        main_metadata["provider"] = provider
                    if not main_metadata.get("model"):
                        main_metadata["model"] = model_name
                
                main_metadata['run_date'] = time.now().strftime("%Y-%m-%d %H:%M:%S")
                
                # Update embedding status
                if embedding is not None:
                    main_metadata['has_embedding'] = True
                elif not regenerate_metadata and existing:
                    # Keep existing embedding status
                    main_metadata['has_embedding'] = existing.get('has_embedding', False)
                else:
                    main_metadata['has_embedding'] = False
                
                if replace_ss:
                    for key, value in main_metadata.items():
                        if isinstance(value, str):
                            main_metadata[key] = value.replace("ÃŸ", "ss")

                # Determine if we need to update the embedding
                # Only update embedding if we generated a new one
                update_embedding = embedding if embedding is not None else None
                
                if existing and not regenerate_metadata:
                    logger.info(f"UUID {uuid} already exists. Updating (embedding: {update_embedding is not None}).")
                    chroma_service.update_image(uuid, main_metadata, embedding=update_embedding)
                elif regenerate_metadata:
                    logger.info(f"UUID {uuid} set to regenerate. Updating (embedding: {update_embedding is not None}).")
                    if chroma_service.get_image(uuid) is not None:
                        chroma_service.update_image(uuid, main_metadata, embedding=update_embedding)
                    else:
                        chroma_service.add_image(uuid, embedding, main_metadata)
                else:
                    # New record
                    if embedding is not None:
                        logger.info(f"UUID {uuid} is new. Indexing with embeddings.")
                    else:
                        logger.info(f"UUID {uuid} is new. Indexing metadata-only entry (no embedding).")
                    chroma_service.add_image(uuid, embedding, main_metadata)
                
                success_count += 1

            except Exception as e:
                logger.error(f"Error processing image {uuid}: {str(e)}", exc_info=True)
                failure_count += 1

        return success_count, failure_count
    except Exception as e:
        logger.error(f"Error during batch processing task: {str(e)}", exc_info=True)
        return 0, total_images
